{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "TypeX.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqxQ725xI1mk"
      },
      "source": [
        "This project uses the **ner_dataset**. Load it using pandas. \n",
        "The data can have NaN (null) values. These are removed by replacing them with the previous value in the same column of the dataframe using `fillna` method"
      ],
      "id": "aqxQ725xI1mk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itqniWA2h1as",
        "outputId": "23bf772b-2d34-46e3-8a26-ddff6dfef9fa"
      },
      "source": [
        "!pip install seqeval==0.0.5\n",
        "!pip install keras==2.2.4\n",
        "!git clone https://www.github.com/keras-team/keras-contrib.git\n",
        "!cd keras-contrib\n",
        "!pip install tensorflow==1.14.0"
      ],
      "id": "itqniWA2h1as",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval==0.0.5 in /usr/local/lib/python3.7/dist-packages (0.0.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.5) (1.19.5)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.4) (1.5.2)\n",
            "fatal: destination path 'keras-contrib' already exists and is not an empty directory.\n",
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.34.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (57.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.4)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbDB22XSImG-"
      },
      "source": [
        "Add all the necessary imports"
      ],
      "id": "lbDB22XSImG-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3113bc9f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from keras_contrib.layers import CRF\n",
        "\n",
        "from keras_contrib.utils import save_load_utils\n",
        "\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "3113bc9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiNDoR2IIY3O"
      },
      "source": [
        "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
        "data = data.fillna(method=\"ffill\")"
      ],
      "id": "DiNDoR2IIY3O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "cb807605",
        "outputId": "1f18bd80-5500-456a-8481-4ea9cf0f2b8b"
      },
      "source": [
        "data.tail(10)"
      ],
      "id": "cb807605",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1048565</th>\n",
              "      <td>Sentence: 47958</td>\n",
              "      <td>impact</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048566</th>\n",
              "      <td>Sentence: 47958</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048567</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>Indian</td>\n",
              "      <td>JJ</td>\n",
              "      <td>B-gpe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048568</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>forces</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048569</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>said</td>\n",
              "      <td>VBD</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048570</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>they</td>\n",
              "      <td>PRP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048571</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>responded</td>\n",
              "      <td>VBD</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048572</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048573</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048574</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>attack</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Sentence #       Word  POS    Tag\n",
              "1048565  Sentence: 47958     impact   NN      O\n",
              "1048566  Sentence: 47958          .    .      O\n",
              "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
              "1048568  Sentence: 47959     forces  NNS      O\n",
              "1048569  Sentence: 47959       said  VBD      O\n",
              "1048570  Sentence: 47959       they  PRP      O\n",
              "1048571  Sentence: 47959  responded  VBD      O\n",
              "1048572  Sentence: 47959         to   TO      O\n",
              "1048573  Sentence: 47959        the   DT      O\n",
              "1048574  Sentence: 47959     attack   NN      O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og1MJBE3Kx6J"
      },
      "source": [
        "Find the total no: of words and tags in the dataset"
      ],
      "id": "Og1MJBE3Kx6J"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c48c8ba1",
        "outputId": "855b8a42-0864-4c43-f5d1-a0da7daf4226"
      },
      "source": [
        "words = list(set(data[\"Word\"].values))\n",
        "words.append(\"ENDPAD\")\n",
        "n_words = len(words); n_words"
      ],
      "id": "c48c8ba1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dd6dbe6",
        "outputId": "dffb046f-8b5a-4571-f588-2834722c5a80"
      },
      "source": [
        "tags = list(set(data[\"Tag\"].values))\n",
        "n_tags = len(tags); n_tags"
      ],
      "id": "5dd6dbe6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LocM3IxbOZKb"
      },
      "source": [
        "The data set is of the form:<br>\n",
        "Sentence X -> Word 1 -> POS1 -> Tag1<br>\n",
        "Sentence X -> Word 2 -> POS2 -> Tag2<br>\n",
        "Sentence X -> Word 3 -> POS3 -> Tag3<br>\n",
        ".<br>\n",
        ".<br>\n",
        "Sentence X+1 -> Word 1 -> POS1 -> Tag1\n",
        "\n",
        "This is converted to:<br>\n",
        "Sentence X -> [(Word1, POS1, Tag1), (Word2, POS2, Tag2), (Word3, POS3, Tag3) . . . . ]]<br>\n",
        "Sentence X+1 -> [(Word1, POS1, Tag1), (Word2, POS2, Tag2), (Word3, POS3, Tag3) . . . . ]]<br>\n",
        ".<br>\n",
        ".<br>"
      ],
      "id": "LocM3IxbOZKb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07765652"
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    #This gets the next grouped sentence i.e allows you to get sentences one by one\n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "id": "07765652",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b6c0e3f"
      },
      "source": [
        "getter = SentenceGetter(data)"
      ],
      "id": "2b6c0e3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b66ad42e",
        "outputId": "df334daa-0e5e-49e0-8f25-f12a30210871"
      },
      "source": [
        "sent = getter.get_next()\n",
        "print(sent)"
      ],
      "id": "b66ad42e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc5d3255"
      },
      "source": [
        "sentences = getter.sentences"
      ],
      "id": "cc5d3255",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eC1Gf4bTF2J"
      },
      "source": [
        "Assign numerical label to individual words and tags. This is creating a dictionary of words to their corresponding position (index + 1) in the list."
      ],
      "id": "7eC1Gf4bTF2J"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1565eab5"
      },
      "source": [
        "max_len = 75\n",
        "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}"
      ],
      "id": "1565eab5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6916e5bb",
        "outputId": "02a08994-0006-4c9b-a023-72c267e1d259"
      },
      "source": [
        "word2idx[\"Obama\"]\n"
      ],
      "id": "6916e5bb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7238"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f3f9916",
        "outputId": "37540933-4027-4bcb-eaa9-b145dfc3fd95"
      },
      "source": [
        "tag2idx"
      ],
      "id": "7f3f9916",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-art': 9,\n",
              " 'B-eve': 12,\n",
              " 'B-geo': 0,\n",
              " 'B-gpe': 6,\n",
              " 'B-nat': 7,\n",
              " 'B-org': 2,\n",
              " 'B-per': 5,\n",
              " 'B-tim': 11,\n",
              " 'I-art': 15,\n",
              " 'I-eve': 1,\n",
              " 'I-geo': 16,\n",
              " 'I-gpe': 3,\n",
              " 'I-nat': 13,\n",
              " 'I-org': 10,\n",
              " 'I-per': 14,\n",
              " 'I-tim': 8,\n",
              " 'O': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4629367c",
        "outputId": "c57036b0-383c-48da-d9ec-658f52502e95"
      },
      "source": [
        "tag2idx[\"B-geo\"]\n"
      ],
      "id": "4629367c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0Wky9Mpm1kh",
        "outputId": "9c871ed1-d66a-4848-b776-8302067c637b"
      },
      "source": [
        "n_words"
      ],
      "id": "M0Wky9Mpm1kh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFsN_JFnUxjb"
      },
      "source": [
        "Now we map the sentences to a sequence of numbers and then pad the sequence. Note that we increased the index of the words by one to use zero as a padding value. This is done because we want to use the `mask_zero` parameter of the embedding layer to ignore inputs with value zero."
      ],
      "id": "sFsN_JFnUxjb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7757df56"
      },
      "source": [
        "X = [[word2idx[w[0]] for w in s] for s in sentences]"
      ],
      "id": "7757df56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39a5e6f1"
      },
      "source": [
        "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)"
      ],
      "id": "39a5e6f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5754c992"
      },
      "source": [
        "y = [[tag2idx[w[2]] for w in s] for s in sentences]"
      ],
      "id": "5754c992",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3673dece"
      },
      "source": [
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])"
      ],
      "id": "3673dece",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTXyFnA2ho65"
      },
      "source": [
        "Perform one-hot encoding on the labels to convert the numerical value into vectors. `to_categorical` from the *Keras* library is used for this purpose. For eg, val=1 is represented as [1 0 0 0 .... 0] (number of places = total number of classes) and val = 2 as [0 1 0 0 0 ... 0] and so on\n",
        "\n"
      ],
      "id": "cTXyFnA2ho65"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85c1c016"
      },
      "source": [
        "y = [to_categorical(i, num_classes=n_tags) for i in y]"
      ],
      "id": "85c1c016",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPAq2biNja9b"
      },
      "source": [
        "Split the dataset into train and test random subset with a test_size of 10%. This is done with the `train_test_split` function in the *sklearn* library"
      ],
      "id": "XPAq2biNja9b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6212a692"
      },
      "source": [
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)"
      ],
      "id": "6212a692",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY1f3O0npWzf"
      },
      "source": [
        "A Tensorflow model is built with Bidiectional LSTM and CRF layers:<br>\n",
        "![image.png](data:image/png;base64,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) <br>\n",
        "\n",
        "The model uses Bidirectional LSTM with CRF(Conditional Random Fields) to understand and pedict the tag of a given word based on its context. <br>\n",
        "Bidirectional LSTM: A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence).\n",
        "<br><br>\n",
        "Optimizer: It decides how to minimize the loss calculated by the Lososs Function by adjusting the weights and biases of the model Here, **RmsProp** optimizer is being used<br>\n",
        "\n",
        "Loss Function: It calculates the difference between the xpected and actual result in each epoch. Here **CRF Loss** function is used<br>\n",
        "\n",
        "Metric: This is used to display the history of the raining process. It tells how the model improves or impairs.\n",
        "\n",
        "\n"
      ],
      "id": "mY1f3O0npWzf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92cb5496",
        "outputId": "16435663-4db9-4b55-a3af-f89f6164abab"
      },
      "source": [
        "input = Input(shape=(max_len,))\n",
        "model = Embedding(input_dim=n_words + 1, output_dim=20,\n",
        "                  input_length=max_len, mask_zero=True)(input)  # 20-dim embedding\n",
        "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
        "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
        "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
        "crf = CRF(n_tags)  # CRF layer\n",
        "out = crf(model)  # output\n",
        "model = Model(input, out)\n",
        "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])"
      ],
      "id": "92cb5496",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkOes6rhrphD"
      },
      "source": [
        "The model is fit(trained) on the training dataset we made earlier. "
      ],
      "id": "pkOes6rhrphD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa0acX82YRxP",
        "outputId": "1a2caeec-2979-49a4-aca2-a27ca9868f0d"
      },
      "source": [
        "history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=5, validation_split=0.1, verbose=1)"
      ],
      "id": "Oa0acX82YRxP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 38846 samples, validate on 4317 samples\n",
            "Epoch 1/5\n",
            "38846/38846 [==============================] - 225s 6ms/step - loss: 8.9702 - crf_viterbi_accuracy: 0.9110 - val_loss: 8.7656 - val_crf_viterbi_accuracy: 0.9513\n",
            "Epoch 2/5\n",
            "38846/38846 [==============================] - 237s 6ms/step - loss: 8.7065 - crf_viterbi_accuracy: 0.9589 - val_loss: 8.7167 - val_crf_viterbi_accuracy: 0.9616\n",
            "Epoch 3/5\n",
            "38846/38846 [==============================] - 231s 6ms/step - loss: 8.6738 - crf_viterbi_accuracy: 0.9664 - val_loss: 8.7001 - val_crf_viterbi_accuracy: 0.9646\n",
            "Epoch 4/5\n",
            "38846/38846 [==============================] - 223s 6ms/step - loss: 8.6598 - crf_viterbi_accuracy: 0.9696 - val_loss: 8.6916 - val_crf_viterbi_accuracy: 0.9667\n",
            "Epoch 5/5\n",
            "38846/38846 [==============================] - 221s 6ms/step - loss: 8.6518 - crf_viterbi_accuracy: 0.9720 - val_loss: 8.6903 - val_crf_viterbi_accuracy: 0.9659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYujnQYdsY12"
      },
      "source": [
        "Use the trained model to predict the tags of sentences in the test dataset"
      ],
      "id": "PYujnQYdsY12"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dKLeFYuaX-O"
      },
      "source": [
        "test_pred = model.predict(X_te, verbose=1)"
      ],
      "id": "5dKLeFYuaX-O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHxZJkKctnFb"
      },
      "source": [
        "Try out the model on a single sentence fom the test dataset. The output from the model are the numerical labels that we converted the input into before. So as these labels are the indices of the actual list of words and tags, it is used to display the words and their corresponding tags by getting the right element from the lists"
      ],
      "id": "JHxZJkKctnFb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poDmSu70alOa"
      },
      "source": [
        "#print(classification_report(test_labels, pred_labels))\n",
        "i = 1928\n",
        "p = model.predict(np.array([X_te[i]]))\n",
        "p = np.argmax(p, axis=-1)\n",
        "true = np.argmax(y_te[i], -1)\n",
        "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\")) # Create a table form with the pipe symbol (|)\n",
        "print(30 * \"=\")\n",
        "for w, t, pred in zip(X_te[i], true, p[0]):\n",
        "    if w != 0:\n",
        "        print(\"{:15}: {:5} {}\".format(words[w-1], tags[t], tags[pred]))"
      ],
      "id": "poDmSu70alOa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVUf7M-jGCOO"
      },
      "source": [
        ""
      ],
      "id": "eVUf7M-jGCOO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtFRJpMcapj4"
      },
      "source": [
        "def predict(sent):\n",
        "  import re\n",
        "  if (type(sent) == str):\n",
        "    test_sentence = re.sub(\"[^\\w]\", \" \", sent).split()\n",
        "  else:\n",
        "    test_sentence = sent\n",
        "    \n",
        "  x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in test_sentence]], padding=\"post\", value=0, maxlen=max_len)\n",
        "\n",
        "  p = model.predict(np.array([x_test_sent[0]]))\n",
        "  p = np.argmax(p, axis=-1)\n",
        "  l = []\n",
        "  for w, pred in zip(test_sentence, p[0]):\n",
        "      if (tags[pred] != 'O'):\n",
        "        l.append(w)\n",
        "  return l"
      ],
      "id": "jtFRJpMcapj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZXibQrTatTs",
        "outputId": "a82e6c0c-fbd3-4344-f190-3965eabd064a"
      },
      "source": [
        "sentence= \"\"\"Janice was a Fellow of the Royal Society, a lifetime member of the \n",
        "Pontifical Academy of Sciences, and a recipient of the Presidential Medal of Freedom, \n",
        "the highest civilian award in the United States. \"\"\"\n",
        "predict(sentence)"
      ],
      "id": "oZXibQrTatTs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Janice',\n",
              " 'Royal',\n",
              " 'Society',\n",
              " 'Pontifical',\n",
              " 'Academy',\n",
              " 'of',\n",
              " 'Sciences',\n",
              " 'Medal',\n",
              " 'United',\n",
              " 'States']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL1iMUbX7oDa"
      },
      "source": [
        "Save the model in *.h5* format so that it'll be available for later use"
      ],
      "id": "pL1iMUbX7oDa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9C1wZi85TE4"
      },
      "source": [
        "filename = \"model1\"\n",
        "save_load_utils.save_all_weights(model,filename)"
      ],
      "id": "g9C1wZi85TE4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEo2qZjixBWh"
      },
      "source": [
        "save_load_utils.load_all_weights(model,\"model1\")"
      ],
      "id": "YEo2qZjixBWh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8YvngHli4jT"
      },
      "source": [
        "!pip install 'h5py==2.10.0' --force-reinstall\n",
        "save_load_utils.load_all_weights(model,\"model1\")"
      ],
      "id": "h8YvngHli4jT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzkCzMiDzN9H"
      },
      "source": [
        ""
      ],
      "id": "QzkCzMiDzN9H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI4j-7xv9Zsb",
        "outputId": "dc34fa88-5840-4cdf-d13b-10c663348b3f"
      },
      "source": [
        "# Importing all required libraries for this task.\n",
        "import nltk\n",
        "import keras\n",
        "nltk.download('punkt')\n",
        "from nltk.util import ngrams\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from itertools import chain\n",
        "import json\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.stem import *\n",
        "from nltk.corpus import wordnet as wn\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')"
      ],
      "id": "RI4j-7xv9Zsb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__NzxCNkU3Vn"
      },
      "source": [
        "# Preprocessing the data"
      ],
      "id": "__NzxCNkU3Vn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gomWgd_UxJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c14284-f391-4151-a425-00aa2e62de5a"
      },
      "source": [
        "def preprocessing():\n",
        "    \"\"\"Loading the data from 'holbrook.txt' and passing to parsing function to get parssed sentences. \n",
        "    Returning the whole dictionary as data.\"\"\"\n",
        "    data = []\n",
        "    \n",
        "    # Reading the txt file\n",
        "    text_file = open(\"./holbrook.txt\")\n",
        "    lines = []\n",
        "    for i in text_file:\n",
        "        lines.append(i.strip())\n",
        "    \n",
        "    # Word tokenizing the sentences\n",
        "    sentences = [nltk.word_tokenize(sent) for sent in lines]\n",
        "    \n",
        "    # Calling a parse function to get corrected, original sentences.\n",
        "    for sent in sentences:\n",
        "        data.append(parsing(sent))\n",
        "    \n",
        "    return data\n",
        "\n",
        "#Calling preprocessing function\n",
        "data = preprocessing()\n",
        "\n",
        "# Testing\n",
        "print(data[2])\n",
        "assert(data[2] == {\n",
        " 'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'],\n",
        " 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'],\n",
        " 'indexes': [9]\n",
        "})\n",
        "\n",
        "# Splitting the data to test 100 lines and remaining training lines\n",
        "test = data[:100]\n",
        "train = data[100:]"
      ],
      "id": "2gomWgd_UxJ_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], 'indexes': [9]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pws2nbD3UePI"
      },
      "source": [
        "# Parsing the data"
      ],
      "id": "Pws2nbD3UePI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xjq3kxB9Zsb"
      },
      "source": [
        "def parsing(sent):  \n",
        "    \"\"\"Parsing the sentence to corrected and original and storing in the dictionary.\"\"\"\n",
        "    loriginal = []\n",
        "    lcorrected = []\n",
        "    indexes = []\n",
        "    cnt = 0\n",
        "    \n",
        "    for i in sent:\n",
        "        if '|' in i:\n",
        "            # Splitting the sentence on '|'\n",
        "            str1 = i.split('|')\n",
        "            # Previous word to '|' is storing in loriginal list.\n",
        "            loriginal.append(str1[0])\n",
        "            # Next word to '|' is storing in lcorrected list.\n",
        "            lcorrected.append(str1[1])\n",
        "            #Noting down the index of error.\n",
        "            indexes.append(cnt)\n",
        "        \n",
        "        else:\n",
        "            # If there is no '|' in sentence, sentence is stored in loriginal and lcorrected as it is.\n",
        "            loriginal.append(i)\n",
        "            lcorrected.append(i)\n",
        "        cnt = cnt+1\n",
        "        \n",
        "    #Loading to loriginal, lcorrected and index list to dictionary.      \n",
        "    dictionary = {'original': loriginal, 'corrected': lcorrected, 'indexes': indexes}\n",
        "    \n",
        "    return dictionary"
      ],
      "id": "6xjq3kxB9Zsb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzyo3ri8U8Jf"
      },
      "source": [
        "#Splitting into train and test set"
      ],
      "id": "Gzyo3ri8U8Jf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "busMPc-f9Zse"
      },
      "source": [
        "# Splitting the data to test - first 100 lines and remaining training lines\n",
        "def test_train_split():\n",
        "    \"\"\"Splitting the data to test - first 100 lines and remaining training lines.\"\"\"\n",
        "    test = data[:100]\n",
        "    train = data[100:]\n",
        "    \n",
        "    # Seperating the train original, test original, test corrected and train corrected from dictionary to list.\n",
        "    train_corrected = [elem['corrected'] for elem in train]\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    test_corrected = [elem['corrected'] for elem in test]\n",
        "    test_original = [elem['original'] for elem in test]\n",
        "    \n",
        "    # Removing all special characters from the list.\n",
        "    test_original = [tokenizer.tokenize(\" \".join(elem)) for elem in test_original]\n",
        "    test_corrected = [tokenizer.tokenize(\" \".join(elem)) for elem in test_corrected]\n",
        "    train_corrected = [tokenizer.tokenize(\" \".join(elem)) for elem in train_corrected]\n",
        "    \n",
        "    return test_corrected, test_original, train_corrected\n",
        "\n",
        "# Test and Training data.\n",
        "test_corrected, test_original, train_corrected = test_train_split()"
      ],
      "id": "busMPc-f9Zse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPErjp3uVCws"
      },
      "source": [
        "# Importing Edit Distance function from nltk"
      ],
      "id": "sPErjp3uVCws"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gu1Jksw9Zsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373d283d-64c9-49b7-9427-4e32b04478ef"
      },
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "# Edit distance returns the number of changes to transform one word to another\n",
        "print(edit_distance(\"hello\", \"hi\"))"
      ],
      "id": "4Gu1Jksw9Zsg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNCf6Bc8VISP"
      },
      "source": [
        "# Getting candidates for replacing a misspelled word"
      ],
      "id": "HNCf6Bc8VISP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzqGtOAT9Zsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4f155a-3e2f-4bac-af7d-c3885d8ab884"
      },
      "source": [
        "def get_candidates(token):\n",
        "    \n",
        "    \"\"\"Get nearest word for a given incorrect word.\"\"\"\n",
        "    doc = []\n",
        "\n",
        "    for i in train_corrected:\n",
        "        doc.append(\" \".join(i))\n",
        "\n",
        "    doc = \" \".join(doc)\n",
        "    doc = nltk.word_tokenize(doc)\n",
        "    unig_freq = nltk.FreqDist(doc)\n",
        "    unique_words = list(unig_freq.keys())\n",
        "\n",
        "    # Calculate distance between two words\n",
        "    s = []\n",
        "    for i in unique_words:\n",
        "        t = edit_distance(i, token)\n",
        "        s.append(t)\n",
        "    \n",
        "    # Store the nearest words in ordered dictionary\n",
        "    dist = dict(zip(unique_words, s))\n",
        "    dist_sorted = dict(sorted(dist.items(), key=lambda x:x[1]))\n",
        "    minimal_dist = list(dist_sorted.values())[0]\n",
        "    \n",
        "    keys_min = list(filter(lambda k: dist_sorted[k] == minimal_dist, dist_sorted.keys()))\n",
        "    \n",
        "    return keys_min\n",
        "\n",
        "print(get_candidates(\"minde\"))"
      ],
      "id": "kzqGtOAT9Zsh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mine', 'mind']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlqHOZL_9Zsi"
      },
      "source": [
        "# This is to culculate unigram and bigram probabilities in correct function\n",
        "doc = []\n",
        "\n",
        "for i in train_corrected:\n",
        "    doc.append(\" \".join(i).lower())\n",
        "\n",
        "doc = \" \".join(doc)\n",
        "doc = nltk.word_tokenize(doc)\n",
        "unig_freq = nltk.FreqDist(doc)\n",
        "unique_words = list(unig_freq.keys())\n",
        "\n",
        "cf_biag = nltk.ConditionalFreqDist(nltk.bigrams(doc))\n",
        "cf_biag = nltk.ConditionalProbDist(cf_biag, nltk.MLEProbDist)"
      ],
      "id": "qlqHOZL_9Zsi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAodh9rB6diI"
      },
      "source": [
        "VBG = Verb Gerund (V-ing) <br>\n",
        "VBD = Verb Past Tense (V-ed) <br>\n",
        "VBN = Verb Past Participle (V-ed) <br>"
      ],
      "id": "BAodh9rB6diI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y282WSuzHYt"
      },
      "source": [
        "def tense(suggestion, sentence):    \n",
        "    \"\"\"Tense Detection\"\"\"\n",
        "    tag = dict(nltk.pos_tag(sentence)).values()\n",
        "    past_tense = ['VBN', 'VBD']\n",
        "    conti_tense = ['VBG']\n",
        "    \n",
        "    # If sentence is past tense append ed and check if it is valid word\n",
        "    if any(x in tag for x in past_tense):\n",
        "        sug = []\n",
        "        for a in suggestion:\n",
        "            if a.lower()+'ed' in unique_words:\n",
        "                sug.append(a+'ed')\n",
        "        for aelem in sug:\n",
        "            suggestion.append(aelem)\n",
        "            \n",
        "    # If sentence is continuous tense append ing and check if it is valid word\n",
        "    if any(x in tag for x in conti_tense):\n",
        "        sug = []\n",
        "        for b in suggestion:\n",
        "            if b.lower()+'ing' in unique_words:\n",
        "                sug.append(b+'ing')\n",
        "        for belem in sug:\n",
        "            suggestion.append(belem)\n",
        "        \n",
        "    return suggestion \n",
        "\n",
        "\n",
        "def named_entity(sentence):\n",
        "  # If any named tag like PERSON, ORGANIZATION or GEOLOCATION append to list.\n",
        "  out = model.predict(sentence)\n",
        "  return out\n",
        "\n",
        "def enn(sentence):\n",
        "    l = []\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(sentence)):\n",
        "        if hasattr(chunk, 'label'):\n",
        "            l.append(' '.join(c[0] for c in chunk))\n",
        "\n",
        "    \n",
        "    if len(l) != 0:\n",
        "        l = \" \".join(l)\n",
        "        l = l.split(\" \")\n",
        "        \n",
        "    return l\n",
        "\n",
        "\n",
        "def word_forms_new(suggest):\n",
        "    \"\"\"Taking different forms of words using derivationally related forms\"\"\"\n",
        "    sug_form = []\n",
        "    for w in suggest:\n",
        "        forms = set()\n",
        "        for i in wn.lemmas(w):\n",
        "            forms.add(i.name())\n",
        "            for j in i.derivationally_related_forms():\n",
        "                forms.add(j.name())\n",
        "        \n",
        "        for a in list(forms):\n",
        "            sug_form.append(a)\n",
        "    \n",
        "    for q in sug_form:\n",
        "        suggest.append(q)\n",
        "    \n",
        "    word_forms = []\n",
        "    [word_forms.append(i) for i in suggest if not i in word_forms]\n",
        "    return word_forms\n",
        "\n",
        "\n",
        "def conditions(corrected, cr_ind):\n",
        "    \"\"\"Common word - Oclock is not detecting. Hence handling manually but not necessary\"\"\"\n",
        "    \n",
        "    if 'oclock' in corrected:\n",
        "        ind = corrected.index('oclock')\n",
        "        corrected = list(map(lambda x: str.replace(x, \"oclock\", \"clock\"), corrected))\n",
        "        corrected.insert(ind, 'o')\n",
        "        return corrected\n",
        "        \n",
        "    return corrected\n",
        "\n",
        "def sentence_sentence_similarity(sentence1):\n",
        "    \"\"\"Sentence - Sentence similarity using sequence matcher\"\"\"\n",
        "    correc = []\n",
        "    for d in train_corrected:\n",
        "        ratio = SequenceMatcher(None, \" \".join(d), \" \".join(sentence1)).ratio()\n",
        "        if ratio > 98:\n",
        "            correc.append(d)\n",
        "    \n",
        "    if len(correc) == 1:\n",
        "        return correc[0]\n",
        "    else:\n",
        "        return []\n"
      ],
      "id": "2y282WSuzHYt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkQwZ7t3VzRf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a25dca8-5632-48be-abe0-9ab0d8488358"
      },
      "source": [
        "\" \".join(\"word\")"
      ],
      "id": "JkQwZ7t3VzRf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'w o r d'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiDBMFgxCPfW"
      },
      "source": [
        ""
      ],
      "id": "QiDBMFgxCPfW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruqGXRroo1fk"
      },
      "source": [
        "def is_stemmed_not_in_dict(i):\n",
        "  stemmer = PorterStemmer()\n",
        "  return stemmer.stem(i) not in wn.words()\n",
        "\n",
        "def is_lemmatized_not_in_dict(i):\n",
        "  return lemmatizer.lemmatize(i.lower()) not in unique_words\n",
        "\n",
        "def is_word_not_in_known_list(i, n_en, sts):\n",
        "  return all([i.lower() not in source for source in [n_en, sts, wn.words()]]) \n",
        "\n",
        "\n",
        "def check_invalid(i, sts, sentence):\n",
        "  n_en = enn(sentence)\n",
        "  return all([is_stemmed_not_in_dict(i), is_lemmatized_not_in_dict(i), \n",
        "              is_word_not_in_known_list(i, n_en, sts), not i.isdigit()])\n",
        "\n",
        "\n",
        "def perform_bigram_distr(suggestion, cnt, sentence):\n",
        "  prob = []\n",
        "  # Bigram probabilities\n",
        "  for sug in suggestion:\n",
        "\n",
        "      # Check the misspelled word is first or last word of the sentence\n",
        "      if ((cnt != 0) and (cnt != len(sentence) - 1)):\n",
        "\n",
        "          try:\n",
        "              p1 = cf_biag[sug.lower()].prob(sentence[cnt + 1].lower())\n",
        "              p2 = cf_biag[corrected[len(corrected) - 1].lower()].prob(sug.lower())\n",
        "              p = p1 * p2\n",
        "              prob.append(p)\n",
        "          except:\n",
        "              prob.append(0)\n",
        "\n",
        "      else:\n",
        "          # If mispelled word is last word of a sencence take probaility of previous word\n",
        "          if cnt == len(sentence) - 1:\n",
        "              try:\n",
        "                  p2 = cf_biag[corrected[len(corrected) - 1].lower()].prob(sug.lower())\n",
        "                  prob.append(p2)\n",
        "              except:\n",
        "                  prob.append(0)\n",
        "\n",
        "\n",
        "          elif cnt == 0:\n",
        "              # If mispelled word is first word of a sencence take probaility of next word\n",
        "              try:\n",
        "                  p1 = cf_biag[sug.lower()].prob(sentence[cnt + 1].lower())\n",
        "                  prob.append(p1)\n",
        "              except:\n",
        "                  prob.append(0)\n",
        "\n",
        "  return prob"
      ],
      "id": "ruqGXRroo1fk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwEOzxVQVW32"
      },
      "source": [
        "# Main function - correct()"
      ],
      "id": "KwEOzxVQVW32"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZmPEEX2jIli"
      },
      "source": [
        "\n",
        "\n",
        "1.   Stemming and Lemmatizaion\n",
        "2.   Sentence Sentence Similarity\n",
        "3. Named Entity Recgn\n",
        "4. Min Edit Distance\n",
        "5. Tense Correction\n",
        "6. Inclusion of alternative word forms\n",
        "7. Bi gram Probability \n",
        "\n"
      ],
      "id": "eZmPEEX2jIli"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZETkvgXki75P"
      },
      "source": [
        ""
      ],
      "id": "ZETkvgXki75P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8m1KmFNzPjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce89245f-9eee-4c73-de48-e41baf68742f"
      },
      "source": [
        "def correct(sentence):\n",
        "    sts = ['oclock']\n",
        "    corrected = []\n",
        "    cnt = 0\n",
        "    indexes = []\n",
        "    #To check stemmed word in dictonary or not\n",
        "    stemmer = PorterStemmer()\n",
        "    status = 0\n",
        "    #This will extract all named entities of a sentence\n",
        "    n_en = enn(sentence)\n",
        "    \n",
        "    for i in sentence:\n",
        "\n",
        "        # Check for sentence similarity\n",
        "        corr = sentence_sentence_similarity(i)\n",
        "        if len(corr) == 1:\n",
        "            return corr\n",
        "        # Ignoring digits like page number and lemmatizing the word and check \n",
        "        # if it is present in dictionary and use words.words() for word validation.\n",
        "       \n",
        "        elif check_invalid(i, sts, sentence):\n",
        "            indexes.append(cnt)\n",
        "            if len(get_candidates(i)) > 1:\n",
        "                # Get words forms, tense detection for suggested sentence\n",
        "                suggestion = get_candidates(i)\n",
        "                suggestion = tense(suggestion, sentence)\n",
        "                wd_fms = word_forms_new(suggestion)\n",
        "                suggestion = wd_fms\n",
        "\n",
        "                prob = perform_bigram_distr(suggestion, cnt, sentence)\n",
        "              \n",
        "                if len(suggestion[prob.index(max(prob))]) > 1:\n",
        "                    corrected.append(suggestion[prob.index(max(prob))])\n",
        "                else:\n",
        "                    corrected.append(suggestion[prob.index(max(prob))])\n",
        "\n",
        "            else:\n",
        "                corrected.append(get_candidates(i)[0])\n",
        "\n",
        "        else:\n",
        "            corrected.append(i)\n",
        "\n",
        "        cnt = cnt+1\n",
        "        # Manula hadling 'Oclock'\n",
        "        corrected = conditions(corrected, indexes)\n",
        "    \n",
        "    fin = sentence_sentence_similarity(corrected)\n",
        "    if len(fin) != 0:\n",
        "        return fin\n",
        "    else:\n",
        "        return corrected\n",
        "\n",
        "\n",
        "\n",
        "correct(['test', 'of', 'goe', 'out', 'some_times'])"
      ],
      "id": "_8m1KmFNzPjB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'of', 'go', 'out', 'sometimes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kySNjB2VzuNi"
      },
      "source": [
        "# Evaluate the Model"
      ],
      "id": "kySNjB2VzuNi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txHjwSSUzkeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c701934e-6311-4d8d-9713-469d36460392"
      },
      "source": [
        "start_time = time.time()\n",
        "def accuracy(actual_sent, sent_pred):\n",
        "    \"\"\"This is based on word to word accuracy calculation. \n",
        "    Compares each word with the actual word and calculate accuracy\"\"\"\n",
        "    actual = actual_sent\n",
        "    predict = correct(sent_pred)\n",
        "    # If the blank sentence i.e for a blank line predicted\n",
        "    # is also blank take accuracy as 1\n",
        "\n",
        "    if len(actual) == 0 and len(predict)==0:\n",
        "        accuracy = 1.0\n",
        "    else:\n",
        "        # Take all predicted words same as actual word \n",
        "        #and divide by lenght of sentence\n",
        "\n",
        "        accuracy = len(set(predict) & set(actual))/len(set(actual))\n",
        "    \n",
        "    return accuracy\n",
        "    \n",
        "acc = []\n",
        "for i in tqdm(range(len(test_corrected))):\n",
        "    acc.append(accuracy(test_corrected[i], test_original[i]))\n",
        "\n",
        "print(\"\\nAverage Accuracy of words in each sentence: \", \n",
        "      round(sum(acc)/len(acc)*100, 4), \"%\")\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"Elapsed Time is: \", elapsed_time)"
      ],
      "id": "txHjwSSUzkeB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|         | 2/100 [00:01<00:59,  1.64it/s]\u001b[A\n",
            "  3%|         | 3/100 [00:02<01:32,  1.04it/s]\u001b[A\n",
            "  4%|         | 4/100 [00:03<01:26,  1.11it/s]\u001b[A\n",
            "  5%|         | 5/100 [00:04<01:31,  1.04it/s]\u001b[A\n",
            "  6%|         | 6/100 [00:06<01:40,  1.07s/it]\u001b[A\n",
            "  7%|         | 7/100 [00:09<02:34,  1.66s/it]\u001b[A\n",
            "  8%|         | 8/100 [00:11<02:44,  1.79s/it]\u001b[A\n",
            "  9%|         | 9/100 [00:12<02:17,  1.51s/it]\u001b[A\n",
            " 10%|         | 10/100 [00:14<02:43,  1.81s/it]\u001b[A\n",
            " 11%|         | 11/100 [00:15<02:17,  1.54s/it]\u001b[A\n",
            " 12%|        | 12/100 [00:18<02:52,  1.96s/it]\u001b[A\n",
            " 13%|        | 13/100 [00:23<03:59,  2.75s/it]\u001b[A\n",
            " 14%|        | 14/100 [00:26<04:06,  2.86s/it]\u001b[A\n",
            " 15%|        | 15/100 [00:27<03:22,  2.39s/it]\u001b[A\n",
            " 16%|        | 16/100 [00:30<03:43,  2.66s/it]\u001b[A\n",
            " 17%|        | 17/100 [00:31<02:46,  2.00s/it]\u001b[A\n",
            " 18%|        | 18/100 [00:32<02:12,  1.62s/it]\u001b[A\n",
            " 19%|        | 19/100 [00:33<02:19,  1.72s/it]\u001b[A\n",
            " 20%|        | 20/100 [00:37<02:56,  2.20s/it]\u001b[A\n",
            " 21%|        | 21/100 [00:38<02:24,  1.83s/it]\u001b[A\n",
            " 22%|       | 22/100 [00:39<02:05,  1.60s/it]\u001b[A\n",
            " 23%|       | 23/100 [00:40<01:49,  1.42s/it]\u001b[A\n",
            " 25%|       | 25/100 [00:42<01:42,  1.36s/it]\u001b[A\n",
            " 26%|       | 26/100 [00:43<01:22,  1.11s/it]\u001b[A\n",
            " 27%|       | 27/100 [00:45<01:46,  1.46s/it]\u001b[A\n",
            " 28%|       | 28/100 [00:47<01:45,  1.47s/it]\u001b[A\n",
            " 29%|       | 29/100 [00:51<02:40,  2.26s/it]\u001b[A\n",
            " 30%|       | 30/100 [00:51<02:07,  1.82s/it]\u001b[A\n",
            " 31%|       | 31/100 [00:54<02:28,  2.15s/it]\u001b[A\n",
            " 32%|      | 32/100 [00:56<02:09,  1.90s/it]\u001b[A\n",
            " 33%|      | 33/100 [00:58<02:07,  1.90s/it]\u001b[A\n",
            " 34%|      | 34/100 [01:03<03:18,  3.01s/it]\u001b[A\n",
            " 35%|      | 35/100 [01:07<03:27,  3.19s/it]\u001b[A\n",
            " 36%|      | 36/100 [01:07<02:32,  2.38s/it]\u001b[A\n",
            " 37%|      | 37/100 [01:11<02:56,  2.79s/it]\u001b[A\n",
            " 38%|      | 38/100 [01:13<02:37,  2.53s/it]\u001b[A\n",
            " 39%|      | 39/100 [01:14<02:11,  2.15s/it]\u001b[A\n",
            " 40%|      | 40/100 [01:17<02:26,  2.45s/it]\u001b[A\n",
            " 41%|      | 41/100 [01:18<01:52,  1.91s/it]\u001b[A\n",
            " 42%|     | 42/100 [01:19<01:29,  1.54s/it]\u001b[A\n",
            " 43%|     | 43/100 [01:21<01:43,  1.82s/it]\u001b[A\n",
            " 44%|     | 44/100 [01:24<01:51,  1.99s/it]\u001b[A\n",
            " 45%|     | 45/100 [01:25<01:32,  1.69s/it]\u001b[A\n",
            " 46%|     | 46/100 [01:26<01:24,  1.56s/it]\u001b[A\n",
            " 47%|     | 47/100 [01:29<01:46,  2.01s/it]\u001b[A\n",
            " 48%|     | 48/100 [01:33<02:10,  2.50s/it]\u001b[A\n",
            " 49%|     | 49/100 [01:36<02:16,  2.67s/it]\u001b[A\n",
            " 50%|     | 50/100 [01:39<02:19,  2.78s/it]\u001b[A\n",
            " 51%|     | 51/100 [01:42<02:29,  3.06s/it]\u001b[A\n",
            " 52%|    | 52/100 [01:44<02:10,  2.71s/it]\u001b[A\n",
            " 53%|    | 53/100 [01:46<01:47,  2.30s/it]\u001b[A\n",
            " 54%|    | 54/100 [01:47<01:31,  1.98s/it]\u001b[A\n",
            " 55%|    | 55/100 [01:49<01:28,  1.96s/it]\u001b[A\n",
            " 56%|    | 56/100 [01:49<01:09,  1.59s/it]\u001b[A\n",
            " 57%|    | 57/100 [01:50<00:55,  1.28s/it]\u001b[A\n",
            " 58%|    | 58/100 [01:51<00:51,  1.23s/it]\u001b[A\n",
            " 59%|    | 59/100 [01:51<00:39,  1.03it/s]\u001b[A\n",
            " 60%|    | 60/100 [01:53<00:42,  1.06s/it]\u001b[A\n",
            " 61%|    | 61/100 [01:53<00:35,  1.11it/s]\u001b[A\n",
            " 62%|   | 62/100 [01:54<00:30,  1.25it/s]\u001b[A\n",
            " 63%|   | 63/100 [01:56<00:44,  1.19s/it]\u001b[A\n",
            " 64%|   | 64/100 [01:57<00:44,  1.24s/it]\u001b[A\n",
            " 65%|   | 65/100 [01:58<00:40,  1.15s/it]\u001b[A\n",
            " 66%|   | 66/100 [02:01<00:57,  1.68s/it]\u001b[A\n",
            " 67%|   | 67/100 [02:04<01:09,  2.12s/it]\u001b[A\n",
            " 68%|   | 68/100 [02:05<00:53,  1.67s/it]\u001b[A\n",
            " 69%|   | 69/100 [02:06<00:45,  1.48s/it]\u001b[A\n",
            " 70%|   | 70/100 [02:08<00:51,  1.73s/it]\u001b[A\n",
            " 71%|   | 71/100 [02:09<00:43,  1.49s/it]\u001b[A\n",
            " 72%|  | 72/100 [02:10<00:31,  1.14s/it]\u001b[A\n",
            " 73%|  | 73/100 [02:11<00:36,  1.35s/it]\u001b[A\n",
            " 74%|  | 74/100 [02:13<00:37,  1.45s/it]\u001b[A\n",
            " 75%|  | 75/100 [02:17<00:54,  2.19s/it]\u001b[A\n",
            " 76%|  | 76/100 [02:19<00:51,  2.13s/it]\u001b[A\n",
            " 77%|  | 77/100 [02:24<01:10,  3.08s/it]\u001b[A\n",
            " 78%|  | 78/100 [02:25<00:55,  2.52s/it]\u001b[A\n",
            " 79%|  | 79/100 [02:27<00:43,  2.09s/it]\u001b[A\n",
            " 80%|  | 80/100 [02:28<00:36,  1.83s/it]\u001b[A\n",
            " 81%|  | 81/100 [02:30<00:37,  2.00s/it]\u001b[A\n",
            " 82%| | 82/100 [02:32<00:34,  1.93s/it]\u001b[A\n",
            " 83%| | 83/100 [02:34<00:33,  1.98s/it]\u001b[A\n",
            " 84%| | 84/100 [02:36<00:30,  1.91s/it]\u001b[A\n",
            " 85%| | 85/100 [02:39<00:32,  2.16s/it]\u001b[A\n",
            " 86%| | 86/100 [02:41<00:31,  2.24s/it]\u001b[A\n",
            " 87%| | 87/100 [02:43<00:28,  2.18s/it]\u001b[A\n",
            " 88%| | 88/100 [02:45<00:24,  2.07s/it]\u001b[A\n",
            " 89%| | 89/100 [02:49<00:30,  2.73s/it]\u001b[A\n",
            " 90%| | 90/100 [02:49<00:20,  2.00s/it]\u001b[A\n",
            " 91%| | 91/100 [02:51<00:17,  1.96s/it]\u001b[A\n",
            " 92%|| 92/100 [02:54<00:17,  2.22s/it]\u001b[A\n",
            " 93%|| 93/100 [02:56<00:15,  2.16s/it]\u001b[A\n",
            " 94%|| 94/100 [02:59<00:13,  2.28s/it]\u001b[A\n",
            " 95%|| 95/100 [03:01<00:11,  2.36s/it]\u001b[A\n",
            " 96%|| 96/100 [03:04<00:09,  2.44s/it]\u001b[A\n",
            " 97%|| 97/100 [03:06<00:07,  2.40s/it]\u001b[A\n",
            " 98%|| 98/100 [03:09<00:05,  2.65s/it]\u001b[A\n",
            " 99%|| 99/100 [03:11<00:02,  2.40s/it]\u001b[A\n",
            "100%|| 100/100 [03:12<00:00,  1.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Average Accuracy of words in each sentence:  89.2142 %\n",
            "Elapsed Time is:  192.83648133277893\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOo9y-kgVeE9"
      },
      "source": [
        "# Testing code"
      ],
      "id": "sOo9y-kgVeE9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S1hbx0v3Lct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a6eb05-52e5-4f47-e21b-dce804c486dc"
      },
      "source": [
        "def test(sent):\n",
        "    print(\"\\nOriginal: \" + sent)\n",
        "    print(\"Corrected: \" + ' '.join(correct(sent.split())))\n",
        "\n",
        "test(\"this is goood\")\n",
        "test(\"I am the best personaaa out therre\")\n",
        "test(\"My show is amaizng\")\n",
        "test(\"look ahedd\")\n",
        "test(\"Adam is whitr in coolor\")\n"
      ],
      "id": "5S1hbx0v3Lct",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Original: this is goood\n",
            "Corrected: this is good\n",
            "\n",
            "Original: I am the best personaaa out therre\n",
            "Corrected: I am the best person out there\n",
            "\n",
            "Original: My show is amaizng\n",
            "Corrected: My show is asking\n",
            "\n",
            "Original: look ahedd\n",
            "Corrected: look ahead\n",
            "\n",
            "Original: Adam is whitr in coolor\n",
            "Corrected: Adam is white in colour\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liFZNOIZ9j4D"
      },
      "source": [
        "Create a function to readily do NER on a random sentence"
      ],
      "id": "liFZNOIZ9j4D"
    }
  ]
}